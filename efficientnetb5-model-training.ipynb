{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013291,
     "end_time": "2020-09-28T12:02:53.336471",
     "exception": false,
     "start_time": "2020-09-28T12:02:53.323180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center><h1>EfficientNet B5</h1> Training</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011912,
     "end_time": "2020-09-28T12:02:53.360711",
     "exception": false,
     "start_time": "2020-09-28T12:02:53.348799",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Commit 0\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- Dropout 0.5\n",
    "\n",
    "**CV MAE is: 3.5531554222106934**\n",
    "\n",
    "### Commit 1\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.001\n",
    "- Dropout 0.385\n",
    "\n",
    "**CV MAE is: 3.9672446727752684**\n",
    "\n",
    "### Commit 2 (Native)\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 456x456\n",
    "- Dropout 0.4\n",
    "- No BatchNorm Layer\n",
    "\n",
    "**CV MAE is: 3.088250017166138**\n",
    "\n",
    "### Commit 3 (Native)\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 456x456\n",
    "- Dropout 0.4\n",
    "- With BatchNorm Layer\n",
    "\n",
    "**CV MAE is: 3.682063627243042**\n",
    "\n",
    "**Since batch size is less, so BatchNorm is not viable, trying Layer Normalization**\n",
    "\n",
    "### Commit 4 (Native)\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 456x456\n",
    "- Dropout 0.4\n",
    "- With LayerNorm Layer\n",
    "\n",
    "**CV MAE is: 3.556714630126953**\n",
    "\n",
    "### Commit 6\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 512x512\n",
    "- Dropout 0.385\n",
    "- 3 Channels\n",
    "\n",
    "**CV MAE is: 3.102118635177612**\n",
    "\n",
    "### Commit 7\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 512x512\n",
    "- Dropout 0.385\n",
    "- 3 Channels\n",
    "- Gaussian Noise 0.25\n",
    "\n",
    "**CV MAE is: 3.195901393890381**\n",
    "\n",
    "### Commit 8\n",
    "- EPOCHS = 100\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 528x528\n",
    "- Dropout 0.385\n",
    "- Gaussian Noise 0.2\n",
    "\n",
    "**CV MAE is: 3.20487517118454**\n",
    "\n",
    "### Commit 9\n",
    "- EPOCHS = 110\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 5\n",
    "- LR = 0.003\n",
    "- 528x528\n",
    "- Dropout 0.4\n",
    "- Gaussian Noise 0.2\n",
    "\n",
    "**CV MAE is: 3.222148895263672**\n",
    "\n",
    "### Commit 10 (10 Folds at 50 Epochs)\n",
    "- EPOCHS = 50\n",
    "- BATCH_SIZE = 4\n",
    "- NFOLD = 10\n",
    "- LR = 0.003\n",
    "- 512x512\n",
    "- Dropout 0.385\n",
    "- Gaussian Noise 0.2\n",
    "- Male\n",
    "\n",
    "**CV MAE is: 3.222148895263672**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-09-28T12:02:53.392496Z",
     "iopub.status.busy": "2020-09-28T12:02:53.391712Z",
     "iopub.status.idle": "2020-09-28T12:03:12.877108Z",
     "shell.execute_reply": "2020-09-28T12:03:12.876597Z"
    },
    "papermill": {
     "duration": 19.504104,
     "end_time": "2020-09-28T12:03:12.877218",
     "exception": false,
     "start_time": "2020-09-28T12:02:53.373114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ./\r\n",
      "Processing /kaggle/input/kerasapplications/keras-team-keras-applications-3b180cb\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (1.18.5)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (2.10.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->Keras-Applications==1.0.8) (1.14.0)\r\n",
      "Building wheels for collected packages: Keras-Applications\r\n",
      "  Building wheel for Keras-Applications (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for Keras-Applications: filename=Keras_Applications-1.0.8-py3-none-any.whl size=50704 sha256=ed8989406b35a76a4657249e32a57fd2b98ff3d67bc61550c0d58c777aca15a0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f4/96/13/eccdd9391bd8df958d78851b98ec4dc207ba05b67b011eb70a\r\n",
      "Successfully built Keras-Applications\r\n",
      "Installing collected packages: Keras-Applications\r\n",
      "Successfully installed Keras-Applications-1.0.8\r\n",
      "Looking in links: ./\r\n",
      "Processing /kaggle/input/efficientnet/efficientnet-1.1.0\r\n",
      "Requirement already satisfied: keras_applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (1.0.8)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.0) (0.16.2)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (2.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.18.5)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.4.1)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (3.2.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.4)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (7.2.0)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (2.8.0)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.0) (1.1.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras_applications<=1.0.8,>=1.0.7->efficientnet==1.1.0) (1.14.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.0) (2.4.7)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.1.0) (4.4.2)\r\n",
      "Building wheels for collected packages: efficientnet\r\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.0-py3-none-any.whl size=14141 sha256=62846ac72bbad8a0269a8c9fe90c377b13a3218d94605a36032759ee8d54214b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/24/f5/31/3cc20871288fe532128224a3f5af7b4d67efb9835bd5683522\r\n",
      "Successfully built efficientnet\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n",
    "!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:12.928283Z",
     "iopub.status.busy": "2020-09-28T12:03:12.927515Z",
     "iopub.status.idle": "2020-09-28T12:03:19.563994Z",
     "shell.execute_reply": "2020-09-28T12:03:19.562977Z"
    },
    "papermill": {
     "duration": 6.664516,
     "end_time": "2020-09-28T12:03:19.564121",
     "exception": false,
     "start_time": "2020-09-28T12:03:12.899605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm.notebook import tqdm \n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, LayerNormalization, GlobalAveragePooling2D, Add, Conv2D, AveragePooling2D, \n",
    "    LeakyReLU, Concatenate \n",
    ")\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.applications as tfa\n",
    "import efficientnet.tfkeras as efn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:19.611796Z",
     "iopub.status.busy": "2020-09-28T12:03:19.611117Z",
     "iopub.status.idle": "2020-09-28T12:03:22.026904Z",
     "shell.execute_reply": "2020-09-28T12:03:22.026381Z"
    },
    "papermill": {
     "duration": 2.441495,
     "end_time": "2020-09-28T12:03:22.027004",
     "exception": false,
     "start_time": "2020-09-28T12:03:19.585509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030992,
     "end_time": "2020-09-28T12:03:22.078921",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.047929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Parameters\n",
    "\n",
    "- `EPOCHS`: number of epochs to train for in each fold\n",
    "- `BATCH_SIZE`: batch size of images during training\n",
    "- `NFOLD`: number of folds in K-fold cross-validation (CV)\n",
    "- `LR`: learning rate\n",
    "- `SAVE_BEST`: default is True to save best weights on validation loss\n",
    "- `MODEL_CLASS`: the class of model. E.g. \"b1\" for EfficientNet-B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.139573Z",
     "iopub.status.busy": "2020-09-28T12:03:22.138985Z",
     "iopub.status.idle": "2020-09-28T12:03:22.142516Z",
     "shell.execute_reply": "2020-09-28T12:03:22.143029Z"
    },
    "papermill": {
     "duration": 0.029931,
     "end_time": "2020-09-28T12:03:22.143136",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.113205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "NFOLD = 10\n",
    "LR = 0.003\n",
    "SAVE_BEST = True\n",
    "MODEL_CLASS = 'b5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.192012Z",
     "iopub.status.busy": "2020-09-28T12:03:22.191189Z",
     "iopub.status.idle": "2020-09-28T12:03:22.202265Z",
     "shell.execute_reply": "2020-09-28T12:03:22.201856Z"
    },
    "papermill": {
     "duration": 0.038846,
     "end_time": "2020-09-28T12:03:22.202391",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.163545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.257599Z",
     "iopub.status.busy": "2020-09-28T12:03:22.256967Z",
     "iopub.status.idle": "2020-09-28T12:03:22.267667Z",
     "shell.execute_reply": "2020-09-28T12:03:22.268210Z"
    },
    "papermill": {
     "duration": 0.045299,
     "end_time": "2020-09-28T12:03:22.268405",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.223106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Weeks</th>\n",
       "      <th>FVC</th>\n",
       "      <th>Percent</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SmokingStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>-4</td>\n",
       "      <td>2315</td>\n",
       "      <td>58.253649</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>5</td>\n",
       "      <td>2214</td>\n",
       "      <td>55.712129</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>7</td>\n",
       "      <td>2061</td>\n",
       "      <td>51.862104</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>9</td>\n",
       "      <td>2144</td>\n",
       "      <td>53.950679</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID00007637202177411956430</td>\n",
       "      <td>11</td>\n",
       "      <td>2069</td>\n",
       "      <td>52.063412</td>\n",
       "      <td>79</td>\n",
       "      <td>Male</td>\n",
       "      <td>Ex-smoker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Patient  Weeks   FVC    Percent  Age   Sex SmokingStatus\n",
       "0  ID00007637202177411956430     -4  2315  58.253649   79  Male     Ex-smoker\n",
       "1  ID00007637202177411956430      5  2214  55.712129   79  Male     Ex-smoker\n",
       "2  ID00007637202177411956430      7  2061  51.862104   79  Male     Ex-smoker\n",
       "3  ID00007637202177411956430      9  2144  53.950679   79  Male     Ex-smoker\n",
       "4  ID00007637202177411956430     11  2069  52.063412   79  Male     Ex-smoker"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.320452Z",
     "iopub.status.busy": "2020-09-28T12:03:22.319491Z",
     "iopub.status.idle": "2020-09-28T12:03:22.324001Z",
     "shell.execute_reply": "2020-09-28T12:03:22.323559Z"
    },
    "papermill": {
     "duration": 0.032964,
     "end_time": "2020-09-28T12:03:22.324088",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.291124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ex-smoker', 'Never smoked', 'Currently smokes'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.SmokingStatus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.375605Z",
     "iopub.status.busy": "2020-09-28T12:03:22.374944Z",
     "iopub.status.idle": "2020-09-28T12:03:22.378728Z",
     "shell.execute_reply": "2020-09-28T12:03:22.378276Z"
    },
    "papermill": {
     "duration": 0.033083,
     "end_time": "2020-09-28T12:03:22.378811",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.345728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tab(df):\n",
    "    vector = [(df.Age.values[0] - 30) / 30] \n",
    "    \n",
    "    if df.Sex.values[0].lower() == 'Male':\n",
    "       vector.append(0)\n",
    "    else:\n",
    "       vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0])\n",
    "    return np.array(vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.428229Z",
     "iopub.status.busy": "2020-09-28T12:03:22.427718Z",
     "iopub.status.idle": "2020-09-28T12:03:22.696282Z",
     "shell.execute_reply": "2020-09-28T12:03:22.695104Z"
    },
    "papermill": {
     "duration": 0.29644,
     "end_time": "2020-09-28T12:03:22.696403",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.399963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c35315d2b24405788068dfdfc1be3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "A = {} \n",
    "TAB = {} \n",
    "P = [] \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
    "    sub = train.loc[train.Patient == p, :] \n",
    "    fvc = sub.FVC.values\n",
    "    weeks = sub.Weeks.values\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.748100Z",
     "iopub.status.busy": "2020-09-28T12:03:22.747266Z",
     "iopub.status.idle": "2020-09-28T12:03:22.749797Z",
     "shell.execute_reply": "2020-09-28T12:03:22.749227Z"
    },
    "papermill": {
     "duration": 0.029918,
     "end_time": "2020-09-28T12:03:22.749890",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.719972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    d = pydicom.dcmread(path)\n",
    "    return cv2.resize((d.pixel_array - d.RescaleIntercept) / (d.RescaleSlope * 1000), (512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:03:22.818230Z",
     "iopub.status.busy": "2020-09-28T12:03:22.800405Z",
     "iopub.status.idle": "2020-09-28T12:05:58.920744Z",
     "shell.execute_reply": "2020-09-28T12:05:58.920176Z"
    },
    "papermill": {
     "duration": 156.148829,
     "end_time": "2020-09-28T12:05:58.920854",
     "exception": false,
     "start_time": "2020-09-28T12:03:22.772025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a15609cc804b30823cca6ab5a0061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for p in tqdm(train.Patient.unique()):\n",
    "    try:\n",
    "        ldir = os.listdir(f'../input/osic-pulmonary-fibrosis-progression-lungs-mask/mask_noise/mask_noise/{p}/')\n",
    "        numb = [float(i[:-4]) for i in ldir]\n",
    "        for i in ldir:\n",
    "            x.append(cv2.imread(f'../input/osic-pulmonary-fibrosis-progression-lungs-mask/mask_noise/mask_noise/{p}/{i}', 0).mean())\n",
    "            y.append(float(i[:-4]) / max(numb))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:05:58.981538Z",
     "iopub.status.busy": "2020-09-28T12:05:58.980741Z",
     "iopub.status.idle": "2020-09-28T12:05:58.983462Z",
     "shell.execute_reply": "2020-09-28T12:05:58.983895Z"
    },
    "papermill": {
     "duration": 0.039982,
     "end_time": "2020-09-28T12:05:58.984007",
     "exception": false,
     "start_time": "2020-09-28T12:05:58.944025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size=BATCH_SIZE):\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {}\n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size)\n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0]\n",
    "                img = get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{k}/{i}')\n",
    "                x.append(img)\n",
    "                a.append(self.a[k])\n",
    "                tab.append(self.tab[k])\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab)\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:05:59.041533Z",
     "iopub.status.busy": "2020-09-28T12:05:59.040903Z",
     "iopub.status.idle": "2020-09-28T12:05:59.044847Z",
     "shell.execute_reply": "2020-09-28T12:05:59.044416Z"
    },
    "papermill": {
     "duration": 0.038191,
     "end_time": "2020-09-28T12:05:59.044931",
     "exception": false,
     "start_time": "2020-09-28T12:05:59.006740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_efficientnet(model, shape):\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 1), model_class=None):\n",
    "    inp = Input(shape=shape)\n",
    "    base = get_efficientnet(model_class, shape)\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    inp2 = Input(shape=(4,))\n",
    "    x2 = tf.keras.layers.GaussianNoise(0.2)(inp2)\n",
    "    x = Concatenate()([x, x2]) \n",
    "    x = Dropout(0.385)(x) \n",
    "    x = Dense(1)(x)\n",
    "    model = Model([inp, inp2] , x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022671,
     "end_time": "2020-09-28T12:05:59.090961",
     "exception": false,
     "start_time": "2020-09-28T12:05:59.068290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-28T12:05:59.148260Z",
     "iopub.status.busy": "2020-09-28T12:05:59.147748Z",
     "iopub.status.idle": "2020-09-28T13:46:23.155726Z",
     "shell.execute_reply": "2020-09-28T13:46:23.157283Z"
    },
    "papermill": {
     "duration": 6024.044004,
     "end_time": "2020-09-28T13:46:23.157506",
     "exception": false,
     "start_time": "2020-09-28T12:05:59.113502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "####### Fold 0 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 7.0313\n",
      "Epoch 00001: val_loss improved from inf to 92420.67188, saving model to fold-0.h5\n",
      "32/32 [==============================] - 22s 697ms/step - loss: 7.0313 - val_loss: 92420.6719\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6631\n",
      "Epoch 00002: val_loss improved from 92420.67188 to 16185.80859, saving model to fold-0.h5\n",
      "32/32 [==============================] - 22s 676ms/step - loss: 4.6631 - val_loss: 16185.8086\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9514\n",
      "Epoch 00003: val_loss improved from 16185.80859 to 4293.44482, saving model to fold-0.h5\n",
      "32/32 [==============================] - 19s 608ms/step - loss: 4.9514 - val_loss: 4293.4448\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2422\n",
      "Epoch 00004: val_loss improved from 4293.44482 to 477.22949, saving model to fold-0.h5\n",
      "32/32 [==============================] - 22s 681ms/step - loss: 4.2422 - val_loss: 477.2295\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9503\n",
      "Epoch 00005: val_loss did not improve from 477.22949\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.9503 - val_loss: 15639.7539\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4581\n",
      "Epoch 00006: val_loss improved from 477.22949 to 244.35130, saving model to fold-0.h5\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 4.4581 - val_loss: 244.3513\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4538\n",
      "Epoch 00007: val_loss improved from 244.35130 to 82.23602, saving model to fold-0.h5\n",
      "32/32 [==============================] - 24s 742ms/step - loss: 4.4538 - val_loss: 82.2360\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4546\n",
      "Epoch 00008: val_loss improved from 82.23602 to 5.42919, saving model to fold-0.h5\n",
      "32/32 [==============================] - 21s 659ms/step - loss: 4.4546 - val_loss: 5.4292\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4381\n",
      "Epoch 00009: val_loss improved from 5.42919 to 5.06479, saving model to fold-0.h5\n",
      "32/32 [==============================] - 19s 605ms/step - loss: 4.4381 - val_loss: 5.0648\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0642\n",
      "Epoch 00010: val_loss did not improve from 5.06479\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.0642 - val_loss: 16.7233\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0349\n",
      "Epoch 00011: val_loss did not improve from 5.06479\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 5.0349 - val_loss: 30.7003\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2167\n",
      "Epoch 00012: val_loss did not improve from 5.06479\n",
      "32/32 [==============================] - 18s 576ms/step - loss: 4.2167 - val_loss: 40.1479\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0080\n",
      "Epoch 00013: val_loss did not improve from 5.06479\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.0080 - val_loss: 104.3183\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6623\n",
      "Epoch 00014: val_loss did not improve from 5.06479\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.6623 - val_loss: 42.5636\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5213\n",
      "Epoch 00015: val_loss improved from 5.06479 to 4.97364, saving model to fold-0.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 4.5213 - val_loss: 4.9736\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7980\n",
      "Epoch 00016: val_loss improved from 4.97364 to 4.30373, saving model to fold-0.h5\n",
      "32/32 [==============================] - 20s 620ms/step - loss: 3.7980 - val_loss: 4.3037\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3497\n",
      "Epoch 00017: val_loss did not improve from 4.30373\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.3497 - val_loss: 7.9860\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8346\n",
      "Epoch 00018: val_loss did not improve from 4.30373\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 3.8346 - val_loss: 4.3703\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7582\n",
      "Epoch 00019: val_loss did not improve from 4.30373\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 3.7582 - val_loss: 5.2672\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0304\n",
      "Epoch 00020: val_loss did not improve from 4.30373\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 5.0304 - val_loss: 4.3223\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5130\n",
      "Epoch 00021: val_loss did not improve from 4.30373\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.5130 - val_loss: 4.5526\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8006\n",
      "Epoch 00022: val_loss improved from 4.30373 to 4.08930, saving model to fold-0.h5\n",
      "32/32 [==============================] - 19s 607ms/step - loss: 4.8006 - val_loss: 4.0893\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1244\n",
      "Epoch 00023: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.1244 - val_loss: 5.1219\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7154\n",
      "Epoch 00024: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 3.7154 - val_loss: 4.8927\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2919\n",
      "Epoch 00025: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.2919 - val_loss: 5.5529\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2458\n",
      "Epoch 00026: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 4.2458 - val_loss: 4.4362\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1619\n",
      "Epoch 00027: val_loss did not improve from 4.08930\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.1619 - val_loss: 4.3344\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1235\n",
      "Epoch 00028: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.1235 - val_loss: 5.1656\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2727\n",
      "Epoch 00029: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 4.2727 - val_loss: 4.1659\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7743\n",
      "Epoch 00030: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 3.7743 - val_loss: 5.1675\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2021\n",
      "Epoch 00031: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.2021 - val_loss: 5.3980\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2525\n",
      "Epoch 00032: val_loss did not improve from 4.08930\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.2525 - val_loss: 5.4899\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9954\n",
      "Epoch 00033: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 3.9954 - val_loss: 4.6429\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8881\n",
      "Epoch 00034: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 3.8881 - val_loss: 4.4833\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3531\n",
      "Epoch 00035: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.3531 - val_loss: 5.4714\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1331\n",
      "Epoch 00036: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.1331 - val_loss: 4.3169\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5039\n",
      "Epoch 00037: val_loss did not improve from 4.08930\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.5039 - val_loss: 5.0873\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4442\n",
      "Epoch 00038: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.4442 - val_loss: 4.8664\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0735\n",
      "Epoch 00039: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 5.0735 - val_loss: 4.9478\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4522\n",
      "Epoch 00040: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.4522 - val_loss: 4.8043\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7701\n",
      "Epoch 00041: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 3.7701 - val_loss: 4.7316\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4884\n",
      "Epoch 00042: val_loss did not improve from 4.08930\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 3.4884 - val_loss: 5.2480\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2798\n",
      "Epoch 00043: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.2798 - val_loss: 4.5205\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1094\n",
      "Epoch 00044: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.1094 - val_loss: 4.9693\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3431\n",
      "Epoch 00045: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.3431 - val_loss: 4.1878\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8463\n",
      "Epoch 00046: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.8463 - val_loss: 5.1438\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2379\n",
      "Epoch 00047: val_loss did not improve from 4.08930\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.2379 - val_loss: 4.8269\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6735\n",
      "Epoch 00048: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 3.6735 - val_loss: 4.9680\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7147\n",
      "Epoch 00049: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 3.7147 - val_loss: 4.2219\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8176\n",
      "Epoch 00050: val_loss did not improve from 4.08930\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 3.8176 - val_loss: 5.4103\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 1 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8387\n",
      "Epoch 00001: val_loss improved from inf to 490447488.00000, saving model to fold-1.h5\n",
      "32/32 [==============================] - 22s 689ms/step - loss: 4.8387 - val_loss: 490447488.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9291\n",
      "Epoch 00002: val_loss improved from 490447488.00000 to 38041.12109, saving model to fold-1.h5\n",
      "32/32 [==============================] - 20s 629ms/step - loss: 4.9291 - val_loss: 38041.1211\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6374\n",
      "Epoch 00003: val_loss improved from 38041.12109 to 33511.06641, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 3.6374 - val_loss: 33511.0664\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4093\n",
      "Epoch 00004: val_loss improved from 33511.06641 to 49.90704, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 4.4093 - val_loss: 49.9070\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4023\n",
      "Epoch 00005: val_loss did not improve from 49.90704\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.4023 - val_loss: 386.5783\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9509\n",
      "Epoch 00006: val_loss improved from 49.90704 to 26.47838, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 602ms/step - loss: 3.9509 - val_loss: 26.4784\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1893\n",
      "Epoch 00007: val_loss improved from 26.47838 to 18.74553, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 4.1893 - val_loss: 18.7455\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5459\n",
      "Epoch 00008: val_loss did not improve from 18.74553\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.5459 - val_loss: 30.7325\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2971\n",
      "Epoch 00009: val_loss did not improve from 18.74553\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.2971 - val_loss: 36977.0508\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2026\n",
      "Epoch 00010: val_loss did not improve from 18.74553\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.2026 - val_loss: 2055.6138\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6880\n",
      "Epoch 00011: val_loss did not improve from 18.74553\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.6880 - val_loss: 2062.8472\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6582\n",
      "Epoch 00012: val_loss did not improve from 18.74553\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.6582 - val_loss: 1267.6895\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9140\n",
      "Epoch 00013: val_loss improved from 18.74553 to 5.83214, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 603ms/step - loss: 3.9140 - val_loss: 5.8321\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3240\n",
      "Epoch 00014: val_loss did not improve from 5.83214\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.3240 - val_loss: 5.9989\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3571\n",
      "Epoch 00015: val_loss improved from 5.83214 to 5.37414, saving model to fold-1.h5\n",
      "32/32 [==============================] - 20s 615ms/step - loss: 4.3571 - val_loss: 5.3741\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3406\n",
      "Epoch 00016: val_loss did not improve from 5.37414\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.3406 - val_loss: 6.4276\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9896\n",
      "Epoch 00017: val_loss improved from 5.37414 to 5.11840, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 3.9896 - val_loss: 5.1184\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6722\n",
      "Epoch 00018: val_loss did not improve from 5.11840\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 3.6722 - val_loss: 5.4878\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7693\n",
      "Epoch 00019: val_loss improved from 5.11840 to 4.76464, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 606ms/step - loss: 4.7693 - val_loss: 4.7646\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8568\n",
      "Epoch 00020: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 3.8568 - val_loss: 6.3083\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5218\n",
      "Epoch 00021: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.5218 - val_loss: 5.1406\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9447\n",
      "Epoch 00022: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 3.9447 - val_loss: 5.4325\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9138\n",
      "Epoch 00023: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 3.9138 - val_loss: 6.9230\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5655\n",
      "Epoch 00024: val_loss did not improve from 4.76464\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.5655 - val_loss: 5.2546\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5291\n",
      "Epoch 00025: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.5291 - val_loss: 5.5852\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.5949\n",
      "Epoch 00026: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 3.5949 - val_loss: 5.5152\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6705\n",
      "Epoch 00027: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.6705 - val_loss: 4.8447\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0738\n",
      "Epoch 00028: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.0738 - val_loss: 6.7739\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4074\n",
      "Epoch 00029: val_loss did not improve from 4.76464\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.4074 - val_loss: 5.9103\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7780\n",
      "Epoch 00030: val_loss did not improve from 4.76464\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.7780 - val_loss: 4.8226\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9072\n",
      "Epoch 00031: val_loss improved from 4.76464 to 4.73450, saving model to fold-1.h5\n",
      "32/32 [==============================] - 19s 607ms/step - loss: 3.9072 - val_loss: 4.7345\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2529\n",
      "Epoch 00032: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.2529 - val_loss: 5.7983\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2704\n",
      "Epoch 00033: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.2704 - val_loss: 5.5850\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1476\n",
      "Epoch 00034: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.1476 - val_loss: 5.7934\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8063\n",
      "Epoch 00035: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 3.8063 - val_loss: 5.5537\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0870\n",
      "Epoch 00036: val_loss did not improve from 4.73450\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.0870 - val_loss: 5.0649\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0568\n",
      "Epoch 00037: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.0568 - val_loss: 5.7085\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.5129\n",
      "Epoch 00038: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 554ms/step - loss: 3.5129 - val_loss: 5.5577\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2032\n",
      "Epoch 00039: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.2032 - val_loss: 5.4172\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5749\n",
      "Epoch 00040: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 4.5749 - val_loss: 5.1824\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8971\n",
      "Epoch 00041: val_loss did not improve from 4.73450\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 3.8971 - val_loss: 6.3198\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2152\n",
      "Epoch 00042: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.2152 - val_loss: 5.8294\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4571\n",
      "Epoch 00043: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.4571 - val_loss: 5.0456\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3180\n",
      "Epoch 00044: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.3180 - val_loss: 4.8720\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0274\n",
      "Epoch 00045: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.0274 - val_loss: 6.4595\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6449\n",
      "Epoch 00046: val_loss did not improve from 4.73450\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 3.6449 - val_loss: 5.2281\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1363\n",
      "Epoch 00047: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.1363 - val_loss: 5.1060\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1800\n",
      "Epoch 00048: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.1800 - val_loss: 5.3564\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4519\n",
      "Epoch 00049: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.4519 - val_loss: 6.1584\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1444\n",
      "Epoch 00050: val_loss did not improve from 4.73450\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.1444 - val_loss: 5.8790\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 2 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8653\n",
      "Epoch 00001: val_loss improved from inf to 8398644.00000, saving model to fold-2.h5\n",
      "32/32 [==============================] - 23s 710ms/step - loss: 4.8653 - val_loss: 8398644.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6114\n",
      "Epoch 00002: val_loss improved from 8398644.00000 to 49502.18359, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 611ms/step - loss: 4.6114 - val_loss: 49502.1836\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3070\n",
      "Epoch 00003: val_loss improved from 49502.18359 to 4756.08008, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 617ms/step - loss: 4.3070 - val_loss: 4756.0801\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4104\n",
      "Epoch 00004: val_loss improved from 4756.08008 to 54.14241, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 627ms/step - loss: 4.4104 - val_loss: 54.1424\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1737\n",
      "Epoch 00005: val_loss did not improve from 54.14241\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.1737 - val_loss: 270.3248\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4191\n",
      "Epoch 00006: val_loss improved from 54.14241 to 32.40054, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 611ms/step - loss: 4.4191 - val_loss: 32.4005\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4040\n",
      "Epoch 00007: val_loss did not improve from 32.40054\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.4040 - val_loss: 326.9484\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9111\n",
      "Epoch 00008: val_loss improved from 32.40054 to 6.23397, saving model to fold-2.h5\n",
      "32/32 [==============================] - 19s 609ms/step - loss: 4.9111 - val_loss: 6.2340\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7126\n",
      "Epoch 00009: val_loss did not improve from 6.23397\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.7126 - val_loss: 6.4323\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6106\n",
      "Epoch 00010: val_loss did not improve from 6.23397\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 4.6106 - val_loss: 48.6206\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0020\n",
      "Epoch 00011: val_loss did not improve from 6.23397\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.0020 - val_loss: 13.7312\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5749\n",
      "Epoch 00012: val_loss did not improve from 6.23397\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.5749 - val_loss: 34.6594\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8573\n",
      "Epoch 00013: val_loss improved from 6.23397 to 5.40883, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 615ms/step - loss: 4.8573 - val_loss: 5.4088\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7840\n",
      "Epoch 00014: val_loss did not improve from 5.40883\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 3.7840 - val_loss: 6.7537\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1859\n",
      "Epoch 00015: val_loss improved from 5.40883 to 5.04090, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 611ms/step - loss: 4.1859 - val_loss: 5.0409\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9326\n",
      "Epoch 00016: val_loss did not improve from 5.04090\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 3.9326 - val_loss: 5.7477\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0803\n",
      "Epoch 00017: val_loss improved from 5.04090 to 4.84403, saving model to fold-2.h5\n",
      "32/32 [==============================] - 19s 605ms/step - loss: 5.0803 - val_loss: 4.8440\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1867\n",
      "Epoch 00018: val_loss did not improve from 4.84403\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.1867 - val_loss: 5.7834\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6429\n",
      "Epoch 00019: val_loss did not improve from 4.84403\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.6429 - val_loss: 5.2423\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1752\n",
      "Epoch 00020: val_loss improved from 4.84403 to 4.15931, saving model to fold-2.h5\n",
      "32/32 [==============================] - 20s 616ms/step - loss: 4.1752 - val_loss: 4.1593\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9463\n",
      "Epoch 00021: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 3.9463 - val_loss: 5.6328\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8921\n",
      "Epoch 00022: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 3.8921 - val_loss: 5.0117\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4510\n",
      "Epoch 00023: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 3.4510 - val_loss: 5.1462\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5404\n",
      "Epoch 00024: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.5404 - val_loss: 9.3372\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4923\n",
      "Epoch 00025: val_loss did not improve from 4.15931\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 18s 553ms/step - loss: 4.4923 - val_loss: 6.5223\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2618\n",
      "Epoch 00026: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.2618 - val_loss: 25.8804\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4866\n",
      "Epoch 00027: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 4.4866 - val_loss: 6.7698\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2079\n",
      "Epoch 00028: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.2079 - val_loss: 4.2943\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2539\n",
      "Epoch 00029: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.2539 - val_loss: 6.1962\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2545\n",
      "Epoch 00030: val_loss did not improve from 4.15931\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.2545 - val_loss: 4.4183\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8560\n",
      "Epoch 00031: val_loss did not improve from 4.15931\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 3.8560 - val_loss: 5.5223\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3855\n",
      "Epoch 00032: val_loss improved from 4.15931 to 4.07145, saving model to fold-2.h5\n",
      "32/32 [==============================] - 19s 609ms/step - loss: 4.3855 - val_loss: 4.0714\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2056\n",
      "Epoch 00033: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 4.2056 - val_loss: 5.2053\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0912\n",
      "Epoch 00034: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 5.0912 - val_loss: 6.0599\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2631\n",
      "Epoch 00035: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.2631 - val_loss: 5.1180\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4964\n",
      "Epoch 00036: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.4964 - val_loss: 4.4694\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0913\n",
      "Epoch 00037: val_loss did not improve from 4.07145\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.0913 - val_loss: 5.6327\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3748\n",
      "Epoch 00038: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.3748 - val_loss: 5.2384\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1672\n",
      "Epoch 00039: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.1672 - val_loss: 5.5216\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2172\n",
      "Epoch 00040: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.2172 - val_loss: 4.7100\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9822\n",
      "Epoch 00041: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 3.9822 - val_loss: 7.1428\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3543\n",
      "Epoch 00042: val_loss did not improve from 4.07145\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 4.3543 - val_loss: 4.6068\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5953\n",
      "Epoch 00043: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.5953 - val_loss: 4.0956\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4190\n",
      "Epoch 00044: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.4190 - val_loss: 5.4547\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6432\n",
      "Epoch 00045: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 3.6432 - val_loss: 4.8509\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.3148\n",
      "Epoch 00046: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 5.3148 - val_loss: 6.0688\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5888\n",
      "Epoch 00047: val_loss did not improve from 4.07145\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.5888 - val_loss: 6.8549\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9311\n",
      "Epoch 00048: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 3.9311 - val_loss: 6.2386\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0339\n",
      "Epoch 00049: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 18s 553ms/step - loss: 4.0339 - val_loss: 5.2820\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.5730\n",
      "Epoch 00050: val_loss did not improve from 4.07145\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.5730 - val_loss: 5.3870\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 3 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.7531\n",
      "Epoch 00001: val_loss improved from inf to 2462084.00000, saving model to fold-3.h5\n",
      "32/32 [==============================] - 22s 692ms/step - loss: 5.7531 - val_loss: 2462084.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7095\n",
      "Epoch 00002: val_loss did not improve from 2462084.00000\n",
      "32/32 [==============================] - 18s 554ms/step - loss: 4.7095 - val_loss: 8926544.0000\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5355\n",
      "Epoch 00003: val_loss improved from 2462084.00000 to 1792216.62500, saving model to fold-3.h5\n",
      "32/32 [==============================] - 19s 605ms/step - loss: 4.5355 - val_loss: 1792216.6250\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7924\n",
      "Epoch 00004: val_loss improved from 1792216.62500 to 3527.85352, saving model to fold-3.h5\n",
      "32/32 [==============================] - 20s 620ms/step - loss: 4.7924 - val_loss: 3527.8535\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9643\n",
      "Epoch 00005: val_loss improved from 3527.85352 to 1834.38696, saving model to fold-3.h5\n",
      "32/32 [==============================] - 20s 619ms/step - loss: 3.9643 - val_loss: 1834.3870\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5353\n",
      "Epoch 00006: val_loss did not improve from 1834.38696\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.5353 - val_loss: 229060.7812\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6710\n",
      "Epoch 00007: val_loss did not improve from 1834.38696\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.6710 - val_loss: 60124.0000\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.2411\n",
      "Epoch 00008: val_loss improved from 1834.38696 to 1255.16931, saving model to fold-3.h5\n",
      "32/32 [==============================] - 20s 619ms/step - loss: 5.2411 - val_loss: 1255.1693\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6640\n",
      "Epoch 00009: val_loss improved from 1255.16931 to 34.65160, saving model to fold-3.h5\n",
      "32/32 [==============================] - 20s 636ms/step - loss: 4.6640 - val_loss: 34.6516\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9026\n",
      "Epoch 00010: val_loss improved from 34.65160 to 3.41801, saving model to fold-3.h5\n",
      "32/32 [==============================] - 19s 605ms/step - loss: 4.9026 - val_loss: 3.4180\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8916\n",
      "Epoch 00011: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.8916 - val_loss: 16.1280\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8984\n",
      "Epoch 00012: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.8984 - val_loss: 3.6221\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7929\n",
      "Epoch 00013: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.7929 - val_loss: 3.6323\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5784\n",
      "Epoch 00014: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.5784 - val_loss: 11.0575\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1404\n",
      "Epoch 00015: val_loss did not improve from 3.41801\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 5.1404 - val_loss: 43.8277\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7756\n",
      "Epoch 00016: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.7756 - val_loss: 138.3749\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2274\n",
      "Epoch 00017: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.2274 - val_loss: 22.5576\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1341\n",
      "Epoch 00018: val_loss did not improve from 3.41801\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.1341 - val_loss: 4.5688\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9319\n",
      "Epoch 00019: val_loss improved from 3.41801 to 2.58036, saving model to fold-3.h5\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 4.9319 - val_loss: 2.5804\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5037\n",
      "Epoch 00020: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.5037 - val_loss: 3.9555\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3894\n",
      "Epoch 00021: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.3894 - val_loss: 3.4483\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1361\n",
      "Epoch 00022: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.1361 - val_loss: 3.1917\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4007\n",
      "Epoch 00023: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.4007 - val_loss: 3.7181\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5828\n",
      "Epoch 00024: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.5828 - val_loss: 3.5993\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5189\n",
      "Epoch 00025: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 4.5189 - val_loss: 3.6420\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4309\n",
      "Epoch 00026: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.4309 - val_loss: 3.1359\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6537\n",
      "Epoch 00027: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.6537 - val_loss: 3.7159\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4900\n",
      "Epoch 00028: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.4900 - val_loss: 3.1450\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7713\n",
      "Epoch 00029: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.7713 - val_loss: 3.2504\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6281\n",
      "Epoch 00030: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 3.6281 - val_loss: 3.6429\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4571\n",
      "Epoch 00031: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 557ms/step - loss: 4.4571 - val_loss: 3.4342\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6385\n",
      "Epoch 00032: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.6385 - val_loss: 3.5344\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.5214\n",
      "Epoch 00033: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 3.5214 - val_loss: 3.6889\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5312\n",
      "Epoch 00034: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.5312 - val_loss: 3.5281\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6279\n",
      "Epoch 00035: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.6279 - val_loss: 3.8095\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5224\n",
      "Epoch 00036: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.5224 - val_loss: 3.6246\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1581\n",
      "Epoch 00037: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.1581 - val_loss: 3.7750\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4315\n",
      "Epoch 00038: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.4315 - val_loss: 3.3085\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.3745\n",
      "Epoch 00039: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 3.3745 - val_loss: 3.6195\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7569\n",
      "Epoch 00040: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.7569 - val_loss: 3.6509\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0212\n",
      "Epoch 00041: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.0212 - val_loss: 3.1374\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3447\n",
      "Epoch 00042: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.3447 - val_loss: 3.6162\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7767\n",
      "Epoch 00043: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.7767 - val_loss: 4.1663\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8622\n",
      "Epoch 00044: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 18s 554ms/step - loss: 4.8622 - val_loss: 4.0760\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4867\n",
      "Epoch 00045: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 553ms/step - loss: 4.4867 - val_loss: 3.3319\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3679\n",
      "Epoch 00046: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.3679 - val_loss: 3.8639\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1885\n",
      "Epoch 00047: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.1885 - val_loss: 3.8557\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2157\n",
      "Epoch 00048: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.2157 - val_loss: 3.5492\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6946\n",
      "Epoch 00049: val_loss did not improve from 2.58036\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.6946 - val_loss: 3.5007\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9745\n",
      "Epoch 00050: val_loss did not improve from 2.58036\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.9745 - val_loss: 4.0072\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 4 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.4009\n",
      "Epoch 00001: val_loss improved from inf to 37399.72656, saving model to fold-4.h5\n",
      "32/32 [==============================] - 22s 700ms/step - loss: 5.4009 - val_loss: 37399.7266\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9431\n",
      "Epoch 00002: val_loss improved from 37399.72656 to 17511.06250, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 602ms/step - loss: 4.9431 - val_loss: 17511.0625\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7049\n",
      "Epoch 00003: val_loss improved from 17511.06250 to 1509.01880, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 4.7049 - val_loss: 1509.0188\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9955\n",
      "Epoch 00004: val_loss did not improve from 1509.01880\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.9955 - val_loss: 58262.4023\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6830\n",
      "Epoch 00005: val_loss improved from 1509.01880 to 1283.79993, saving model to fold-4.h5\n",
      "32/32 [==============================] - 20s 610ms/step - loss: 4.6830 - val_loss: 1283.7999\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7724\n",
      "Epoch 00006: val_loss improved from 1283.79993 to 13.74616, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 608ms/step - loss: 4.7724 - val_loss: 13.7462\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4829\n",
      "Epoch 00007: val_loss did not improve from 13.74616\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.4829 - val_loss: 8687.1270\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0958\n",
      "Epoch 00008: val_loss did not improve from 13.74616\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.0958 - val_loss: 421.9804\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7187\n",
      "Epoch 00009: val_loss did not improve from 13.74616\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 3.7187 - val_loss: 191.7693\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8237\n",
      "Epoch 00010: val_loss did not improve from 13.74616\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 3.8237 - val_loss: 37.0970\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3468\n",
      "Epoch 00011: val_loss improved from 13.74616 to 4.88079, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 604ms/step - loss: 4.3468 - val_loss: 4.8808\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6905\n",
      "Epoch 00012: val_loss did not improve from 4.88079\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.6905 - val_loss: 13.2428\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6708\n",
      "Epoch 00013: val_loss improved from 4.88079 to 4.77171, saving model to fold-4.h5\n",
      "32/32 [==============================] - 20s 611ms/step - loss: 4.6708 - val_loss: 4.7717\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8078\n",
      "Epoch 00014: val_loss did not improve from 4.77171\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.8078 - val_loss: 27577.1777\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4816\n",
      "Epoch 00015: val_loss did not improve from 4.77171\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.4816 - val_loss: 18592.1992\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7185\n",
      "Epoch 00016: val_loss did not improve from 4.77171\n",
      "32/32 [==============================] - 17s 539ms/step - loss: 4.7185 - val_loss: 6.2841\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7820\n",
      "Epoch 00017: val_loss did not improve from 4.77171\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.7820 - val_loss: 9.5479\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0993\n",
      "Epoch 00018: val_loss did not improve from 4.77171\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.0993 - val_loss: 7.8402\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2211\n",
      "Epoch 00019: val_loss improved from 4.77171 to 4.33451, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 608ms/step - loss: 4.2211 - val_loss: 4.3345\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2031\n",
      "Epoch 00020: val_loss improved from 4.33451 to 4.12757, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 602ms/step - loss: 4.2031 - val_loss: 4.1276\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3413\n",
      "Epoch 00021: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.3413 - val_loss: 4.4382\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4692\n",
      "Epoch 00022: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 539ms/step - loss: 4.4692 - val_loss: 4.2716\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1087\n",
      "Epoch 00023: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 18s 555ms/step - loss: 4.1087 - val_loss: 4.3783\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7341\n",
      "Epoch 00024: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 3.7341 - val_loss: 5.2744\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0707\n",
      "Epoch 00025: val_loss did not improve from 4.12757\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.0707 - val_loss: 5.0353\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1707\n",
      "Epoch 00026: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.1707 - val_loss: 4.3783\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2773\n",
      "Epoch 00027: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.2773 - val_loss: 4.5797\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3432\n",
      "Epoch 00028: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.3432 - val_loss: 4.7220\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.6417\n",
      "Epoch 00029: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.6417 - val_loss: 5.8038\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6554\n",
      "Epoch 00030: val_loss did not improve from 4.12757\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.6554 - val_loss: 4.8250\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8967\n",
      "Epoch 00031: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.8967 - val_loss: 4.5409\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4498\n",
      "Epoch 00032: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.4498 - val_loss: 5.6389\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0703\n",
      "Epoch 00033: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.0703 - val_loss: 4.4124\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.7826\n",
      "Epoch 00034: val_loss did not improve from 4.12757\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 3.7826 - val_loss: 4.1611\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6353\n",
      "Epoch 00035: val_loss improved from 4.12757 to 4.10718, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 599ms/step - loss: 4.6353 - val_loss: 4.1072\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7664\n",
      "Epoch 00036: val_loss did not improve from 4.10718\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.7664 - val_loss: 4.8757\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3082\n",
      "Epoch 00037: val_loss did not improve from 4.10718\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.3082 - val_loss: 4.7551\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.8137\n",
      "Epoch 00038: val_loss did not improve from 4.10718\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 3.8137 - val_loss: 4.2161\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9456\n",
      "Epoch 00039: val_loss improved from 4.10718 to 3.86308, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 598ms/step - loss: 3.9456 - val_loss: 3.8631\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5348\n",
      "Epoch 00040: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.5348 - val_loss: 4.0565\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7502\n",
      "Epoch 00041: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 4.7502 - val_loss: 4.0209\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0363\n",
      "Epoch 00042: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.0363 - val_loss: 4.4768\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4339\n",
      "Epoch 00043: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.4339 - val_loss: 5.1917\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6325\n",
      "Epoch 00044: val_loss did not improve from 3.86308\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.6325 - val_loss: 4.5991\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0806\n",
      "Epoch 00045: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 4.0806 - val_loss: 5.1562\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1443\n",
      "Epoch 00046: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.1443 - val_loss: 4.3038\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2205\n",
      "Epoch 00047: val_loss did not improve from 3.86308\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.2205 - val_loss: 4.6663\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1008\n",
      "Epoch 00048: val_loss improved from 3.86308 to 3.76389, saving model to fold-4.h5\n",
      "32/32 [==============================] - 19s 599ms/step - loss: 4.1008 - val_loss: 3.7639\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6615\n",
      "Epoch 00049: val_loss did not improve from 3.76389\n",
      "32/32 [==============================] - 18s 551ms/step - loss: 4.6615 - val_loss: 4.3220\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7312\n",
      "Epoch 00050: val_loss did not improve from 3.76389\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.7312 - val_loss: 5.3165\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 5 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3779\n",
      "Epoch 00001: val_loss improved from inf to 781520.43750, saving model to fold-5.h5\n",
      "32/32 [==============================] - 22s 701ms/step - loss: 4.3779 - val_loss: 781520.4375\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.1879\n",
      "Epoch 00002: val_loss improved from 781520.43750 to 166.59967, saving model to fold-5.h5\n",
      "32/32 [==============================] - 19s 601ms/step - loss: 5.1879 - val_loss: 166.5997\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9178\n",
      "Epoch 00003: val_loss did not improve from 166.59967\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 4.9178 - val_loss: 1556.1449\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1614\n",
      "Epoch 00004: val_loss did not improve from 166.59967\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 4.1614 - val_loss: 152055.8438\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5348\n",
      "Epoch 00005: val_loss did not improve from 166.59967\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.5348 - val_loss: 30258.6641\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4753\n",
      "Epoch 00006: val_loss did not improve from 166.59967\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.4753 - val_loss: 4899.7188\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7607\n",
      "Epoch 00007: val_loss did not improve from 166.59967\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.001500000013038516.\n",
      "32/32 [==============================] - 17s 540ms/step - loss: 4.7607 - val_loss: 604.4622\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0570\n",
      "Epoch 00008: val_loss improved from 166.59967 to 127.87937, saving model to fold-5.h5\n",
      "32/32 [==============================] - 19s 600ms/step - loss: 4.0570 - val_loss: 127.8794\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5189\n",
      "Epoch 00009: val_loss improved from 127.87937 to 79.20241, saving model to fold-5.h5\n",
      "32/32 [==============================] - 20s 614ms/step - loss: 4.5189 - val_loss: 79.2024\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3131\n",
      "Epoch 00010: val_loss improved from 79.20241 to 27.76499, saving model to fold-5.h5\n",
      "32/32 [==============================] - 20s 623ms/step - loss: 4.3131 - val_loss: 27.7650\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9846\n",
      "Epoch 00011: val_loss improved from 27.76499 to 2.78443, saving model to fold-5.h5\n",
      "32/32 [==============================] - 19s 599ms/step - loss: 4.9846 - val_loss: 2.7844\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.9802\n",
      "Epoch 00012: val_loss improved from 2.78443 to 2.21803, saving model to fold-5.h5\n",
      "32/32 [==============================] - 20s 616ms/step - loss: 4.9802 - val_loss: 2.2180\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6957\n",
      "Epoch 00013: val_loss did not improve from 2.21803\n",
      "32/32 [==============================] - 17s 539ms/step - loss: 4.6957 - val_loss: 3.3753\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.4605\n",
      "Epoch 00014: val_loss did not improve from 2.21803\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.4605 - val_loss: 2.8714\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5987\n",
      "Epoch 00015: val_loss did not improve from 2.21803\n",
      "32/32 [==============================] - 17s 539ms/step - loss: 4.5987 - val_loss: 2.5551\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2961\n",
      "Epoch 00016: val_loss did not improve from 2.21803\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.2961 - val_loss: 2.7914\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7064\n",
      "Epoch 00017: val_loss did not improve from 2.21803\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.000750000006519258.\n",
      "32/32 [==============================] - 17s 538ms/step - loss: 4.7064 - val_loss: 2.3426\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.3717\n",
      "Epoch 00018: val_loss improved from 2.21803 to 1.81702, saving model to fold-5.h5\n",
      "32/32 [==============================] - 19s 601ms/step - loss: 5.3717 - val_loss: 1.8170\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7518\n",
      "Epoch 00019: val_loss improved from 1.81702 to 1.62579, saving model to fold-5.h5\n",
      "32/32 [==============================] - 19s 599ms/step - loss: 4.7518 - val_loss: 1.6258\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6060\n",
      "Epoch 00020: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.6060 - val_loss: 1.8988\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0060\n",
      "Epoch 00021: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 5.0060 - val_loss: 2.8716\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.8475\n",
      "Epoch 00022: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.8475 - val_loss: 2.7004\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0408\n",
      "Epoch 00023: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 538ms/step - loss: 4.0408 - val_loss: 2.3432\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3807\n",
      "Epoch 00024: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.000375000003259629.\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.3807 - val_loss: 2.2030\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0067\n",
      "Epoch 00025: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.0067 - val_loss: 1.6575\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9541\n",
      "Epoch 00026: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 3.9541 - val_loss: 2.3009\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5257\n",
      "Epoch 00027: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.5257 - val_loss: 2.2461\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0704\n",
      "Epoch 00028: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 542ms/step - loss: 4.0704 - val_loss: 2.2946\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9933\n",
      "Epoch 00029: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001875000016298145.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 3.9933 - val_loss: 2.2379\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.9790\n",
      "Epoch 00030: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 538ms/step - loss: 3.9790 - val_loss: 2.5303\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0501\n",
      "Epoch 00031: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 5.0501 - val_loss: 2.4031\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5944\n",
      "Epoch 00032: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 538ms/step - loss: 4.5944 - val_loss: 2.1913\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3921\n",
      "Epoch 00033: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.3921 - val_loss: 1.8431\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0048\n",
      "Epoch 00034: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 9.375000081490725e-05.\n",
      "32/32 [==============================] - 17s 537ms/step - loss: 5.0048 - val_loss: 2.2633\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3615\n",
      "Epoch 00035: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.3615 - val_loss: 2.0507\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 3.4948\n",
      "Epoch 00036: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 538ms/step - loss: 3.4948 - val_loss: 2.4923\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.1700\n",
      "Epoch 00037: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 545ms/step - loss: 4.1700 - val_loss: 2.3531\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5349\n",
      "Epoch 00038: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 544ms/step - loss: 4.5349 - val_loss: 2.6182\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5372\n",
      "Epoch 00039: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.6875000407453626e-05.\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.5372 - val_loss: 2.3999\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.5910\n",
      "Epoch 00040: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.5910 - val_loss: 2.3518\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2768\n",
      "Epoch 00041: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 547ms/step - loss: 4.2768 - val_loss: 1.9334\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3281\n",
      "Epoch 00042: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 549ms/step - loss: 4.3281 - val_loss: 2.4628\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.7010\n",
      "Epoch 00043: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 543ms/step - loss: 4.7010 - val_loss: 2.2363\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.0587\n",
      "Epoch 00044: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.3437500203726813e-05.\n",
      "32/32 [==============================] - 17s 546ms/step - loss: 4.0587 - val_loss: 2.2156\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3785\n",
      "Epoch 00045: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.3785 - val_loss: 2.1851\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.2257\n",
      "Epoch 00046: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 541ms/step - loss: 4.2257 - val_loss: 2.4586\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3519\n",
      "Epoch 00047: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 18s 550ms/step - loss: 4.3519 - val_loss: 2.3360\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3050\n",
      "Epoch 00048: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 547ms/step - loss: 4.3050 - val_loss: 2.3086\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.3308\n",
      "Epoch 00049: val_loss did not improve from 1.62579\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.1718750101863407e-05.\n",
      "32/32 [==============================] - 18s 548ms/step - loss: 4.3308 - val_loss: 2.6045\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 4.6748\n",
      "Epoch 00050: val_loss did not improve from 1.62579\n",
      "32/32 [==============================] - 17s 539ms/step - loss: 4.6748 - val_loss: 2.2603\n",
      "Training done!\n",
      "#####################\n",
      "####### Fold 6 ######\n",
      "#####################\n",
      "Training...\n",
      "Epoch 1/50\n",
      " 1/32 [..............................] - ETA: 0s - loss: 5.4452"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[4,2048,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_13/efficientnet-b5/top_activation/mul (defined at /opt/conda/lib/python3.7/site-packages/efficientnet/model.py:115) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1142042]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_13/efficientnet-b5/top_activation/mul:\n functional_13/efficientnet-b5/top_bn/FusedBatchNormV3 (defined at <ipython-input-14-156379352ce4>:47)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-156379352ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                         epochs=EPOCHS)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mfolds_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[4,2048,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_13/efficientnet-b5/top_activation/mul (defined at /opt/conda/lib/python3.7/site-packages/efficientnet/model.py:115) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1142042]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_13/efficientnet-b5/top_activation/mul:\n functional_13/efficientnet-b5/top_bn/FusedBatchNormV3 (defined at <ipython-input-14-156379352ce4>:47)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=NFOLD, random_state=42,shuffle=False)\n",
    "P = np.array(P)\n",
    "subs = []\n",
    "folds_history = []\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(P)):\n",
    "    print('#####################')\n",
    "    print('####### Fold %i ######'%fold)\n",
    "    print('#####################')\n",
    "    print('Training...')\n",
    "    \n",
    "    er = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-3,\n",
    "        patience=10,\n",
    "        verbose=1,\n",
    "        mode=\"auto\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    cpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='fold-%i.h5'%fold,\n",
    "        monitor='val_loss', \n",
    "        verbose=1, \n",
    "        save_best_only=SAVE_BEST,\n",
    "        mode='auto'\n",
    "    )\n",
    "\n",
    "    rlp = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.5,\n",
    "        patience=5, \n",
    "        verbose=1, \n",
    "        min_lr=1e-8\n",
    "    )\n",
    "    model = build_model(model_class=MODEL_CLASS)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mae\") \n",
    "    history = model.fit_generator(IGenerator(keys=P[tr_idx], \n",
    "                                   a = A, \n",
    "                                   tab = TAB), \n",
    "                        steps_per_epoch = 32,\n",
    "                        validation_data=IGenerator(keys=P[val_idx], \n",
    "                                   a = A, \n",
    "                                   tab = TAB),\n",
    "                        validation_steps = 16, \n",
    "                        callbacks = [cpt, rlp], \n",
    "                        epochs=EPOCHS)\n",
    "    folds_history.append(history.history)\n",
    "    print('Training done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.992916,
     "end_time": "2020-09-28T13:46:29.124253",
     "exception": false,
     "start_time": "2020-09-28T13:46:26.131337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-09-28T13:46:35.224750Z",
     "iopub.status.busy": "2020-09-28T13:46:35.224007Z",
     "iopub.status.idle": "2020-09-28T13:46:35.228402Z",
     "shell.execute_reply": "2020-09-28T13:46:35.228785Z"
    },
    "papermill": {
     "duration": 3.200378,
     "end_time": "2020-09-28T13:46:35.228909",
     "exception": false,
     "start_time": "2020-09-28T13:46:32.028531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our mean CV MAE is: 3.4775497118631997\n"
     ]
    }
   ],
   "source": [
    "if SAVE_BEST:\n",
    "    mean_val_loss = np.mean([np.min(h['val_loss']) for h in folds_history])\n",
    "else:\n",
    "    mean_val_loss = np.mean([h['val_loss'][-1] for h in folds_history])\n",
    "print('Our mean CV MAE is: ' + str(mean_val_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 6230.455068,
   "end_time": "2020-09-28T13:46:39.938755",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-28T12:02:49.483687",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "028cd67d489246e1816c8620e13229af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2382ae58bdaa4721a363b1361119e188": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7b801dcc04c459795976c512bfd8e4d",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d769ee74ec6d4f5391e1640d95a333f0",
       "value": 1.0
      }
     },
     "23a15609cc804b30823cca6ab5a0061f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4d585b368754132a597f9724631965c",
        "IPY_MODEL_2442acd6a9504147ae4cb0604e55e4ca"
       ],
       "layout": "IPY_MODEL_89c55c056c684d3eb07c1afd376c4354"
      }
     },
     "2442acd6a9504147ae4cb0604e55e4ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6cf8daa90b5445c1bfef0f555e2eb693",
       "placeholder": "",
       "style": "IPY_MODEL_61ccc6608dab46ab9d0d32cab4664d12",
       "value": " 176/176 [02:36&lt;00:00,  1.13it/s]"
      }
     },
     "4110ee11fe624d89936f9104c8a0e972": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f6187062e16461b99fe10e0a73e3bc2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "61ccc6608dab46ab9d0d32cab4664d12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c35315d2b24405788068dfdfc1be3ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2382ae58bdaa4721a363b1361119e188",
        "IPY_MODEL_990420d76cb243748603cfa6f52e167d"
       ],
       "layout": "IPY_MODEL_4110ee11fe624d89936f9104c8a0e972"
      }
     },
     "6cf8daa90b5445c1bfef0f555e2eb693": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "89c55c056c684d3eb07c1afd376c4354": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "990420d76cb243748603cfa6f52e167d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eaf3928e2b054314b8d756f53b86b9c4",
       "placeholder": "",
       "style": "IPY_MODEL_028cd67d489246e1816c8620e13229af",
       "value": " 176/? [02:36&lt;00:00,  1.12it/s]"
      }
     },
     "a90a8634593c413683eb2f4d114ab067": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d769ee74ec6d4f5391e1640d95a333f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "eaf3928e2b054314b8d756f53b86b9c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4d585b368754132a597f9724631965c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a90a8634593c413683eb2f4d114ab067",
       "max": 176.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4f6187062e16461b99fe10e0a73e3bc2",
       "value": 176.0
      }
     },
     "f7b801dcc04c459795976c512bfd8e4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
